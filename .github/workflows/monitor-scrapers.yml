name: Daily Scraper Monitor

on:
  schedule:
    # Runs once a day at 01:00 UTC to check if all scrapers ran
    - cron: '0 1 * * *'
  workflow_dispatch:

jobs:
  monitor-scrapers:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests python-dotenv supabase
        
    - name: Check scraper results
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      run: |
        python -c "
        import os
        from datetime import datetime, timedelta
        from supabase import create_client
        
        # Connect to Supabase
        supabase = create_client(os.getenv('SUPABASE_URL'), os.getenv('SUPABASE_KEY'))
        
        # Check for articles added in the last 24 hours
        yesterday = (datetime.now() - timedelta(days=1)).isoformat()
        
        try:
            result = supabase.table('deals').select('*').gte('created_at', yesterday).execute()
            count = len(result.data)
            
            print(f'✅ Found {count} new articles in the last 24 hours')
            
            # Also check Layer 2 government intelligence discoveries
            try:
                gov_result = supabase.table('companies').select('*').eq('data_sources', 'government').gte('created_at', yesterday).execute()
                gov_count = len(gov_result.data)
                print(f'🏛️ Found {gov_count} new government intelligence discoveries')
            except Exception as e:
                print(f'⚠️ Could not check government discoveries: {e}')
            
            # Check VC portfolio updates
            try:
                vc_result = supabase.table('companies').select('*').eq('data_sources', 'vc_portfolio').gte('updated_at', yesterday).execute()
                vc_count = len(vc_result.data)
                print(f'💼 Found {vc_count} VC portfolio updates')
            except Exception as e:
                print(f'⚠️ Could not check VC portfolio: {e}')
            
            if count == 0:
                print('⚠️ WARNING: No new articles found in the last 24 hours!')
                exit(1)
            else:
                print('🎉 Layer 2 Enhanced Discovery System is working correctly!')
                
        except Exception as e:
            print(f'❌ Error checking database: {e}')
            exit(1)
        "
        
    - name: Create status badge
      run: |
        echo "Last successful run: $(date)" > scraper_status.txt
        echo "Status: ✅ Running" >> scraper_status.txt
